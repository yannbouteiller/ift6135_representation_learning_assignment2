batch_size    128
best_val    None
code_file    randomSearch.py
data    data
debug    False
dp_keep_prob    1
early_stopped    No
emb_size    200
evaluate    False
hidden_size    256
initial_lr    0.0005
model    TRANSFORMER
n_heads    32
nb_searches    5
num_epochs    40
num_layers    8
optimizer    ADAM
patience    3
save_best    True
save_dir    TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_emb_size=200_hidden_size=256_num_layers=8_num_epochs=40_save_best_nb_searches=5_0
seed    1111
seq_len    10
