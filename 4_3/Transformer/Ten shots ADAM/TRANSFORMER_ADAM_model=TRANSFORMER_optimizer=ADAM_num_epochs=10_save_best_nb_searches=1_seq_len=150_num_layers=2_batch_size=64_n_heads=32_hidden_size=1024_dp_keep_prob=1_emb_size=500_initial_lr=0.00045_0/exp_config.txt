batch_size    64
best_val    None
code_file    randomSearch.py
data    data
debug    False
dp_keep_prob    1.0
early_stopped    No
emb_size    500
evaluate    False
hidden_size    1024
initial_lr    0.00045
model    TRANSFORMER
n_heads    32
nb_searches    1
num_epochs    10
num_layers    2
optimizer    ADAM
patience    3
save_best    True
save_dir    TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_num_epochs=10_save_best_nb_searches=1_seq_len=150_num_layers=2_batch_size=64_n_heads=32_hidden_size=1024_dp_keep_prob=1_emb_size=500_initial_lr=0.00045_0
seed    1111
seq_len    150
